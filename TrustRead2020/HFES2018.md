## [HFES2018](HFES2018.md) 62nd International Annual Meeting of the Human Factors and Ergonomics Society
Link: [[Proceedings](https://journals.sagepub.com/toc/proe/62/1)]
[[Conference](https://www.hfes.org/events/2018-hfes-international-annual-meeting)]
Awardなし  
***
**_Public Safety Communication User Needs: Voices of First Responders_**  
Dawkins, Shaneé; Greene, Kristen; Steves, Michelle; Theofanos, Mary; Choong, Yee-Yin; Furman, Susanne; Prettyman, Sandra Spickard  
https://doi.org/10.1177/1541931218621021  
<details><summary>Abstract</summary>The public safety community is transitioning from land mobile radios to a communications technology ecosystem including a variety of broadband data sharing platforms. Successful deployment and adoption of new communications technology relies on efficient and effective user interfaces based on understanding first responder needs, requirements, and contexts of use; human factors research is needed to examine these factors. As such, this paper presents initial qualitative research results via semi-structured interviews with 133 first responders across the U.S. While there are similarities across disciplines, results show there is no easy “one size fits all” communications technology solution. To facilitate trust in new communications technology, solutions must be dependable, easy to use for first responders, and meet their communication needs through the application of user-centered design principles. During this shift in public safety communications technology, the time is now to leverage existing human factors expertise to influence emerging technology for public safety.</details>

***

**_Trust dynamics in sequential decision making_**  
Kim, Changhoon; Zhang, Mengyuan; Zhang2, Chongjie; Yang, X. Jessie  
https://doi.org/10.1177/1541931218621038  
<details><summary>Abstract</summary></details>

***

**_Conveying Automation Reliability and Automation Error Type An Empirical Study in the Cyber Domain_**  
Chen, Jing; Mishler, Scott; Hu, Bin  
https://doi.org/10.1177/1541931218621040  
<details><summary>Abstract</summary>BackgroundEmails have become an integral part of our daily life and work. Phishing emails are often disguised as trustworthy ones and attempt to obtain sensitive information for malicious reasons (Egelman, Cranor, Hong, 2008;). Anti-phishing tools have been designed to help users detect phishing emails or websites (Egelman, et al., 2008; Yang, Xiong, Chen, Proctor, & Li, 2017). However, like any other types of automation aids, these tools are not perfect. An anti-phishing system can make errors, such as labeling a legitimate email as phishing (i.e., a false alarm) or assuming a phishing email as legitimate (i.e., a miss).Human trust in automation has been widely studied as it affects how the human operator interacts with the automation system, which consequently influences the overall system performance (Dzindolet, Peterson, Pomranky, Pierce, & Beck, 2003; Lee & Moray, 1992; Muir, 1994; Sheridan & Parasuraman, 2006). With interacting with an automation system, the human operator should calibrate his or her trust level to trust a system that is capable but distrust a system that is incapable (i.e., trust calibration; Lee & Moray, 1994; Lee & See, 2004; McGuirl & Sarter, 2006).Among the various system capabilities, automation reliability is one of the most important factors that affect trust, and it is widely accepted that higher reliability levels lead to higher trust levels (Desai et al., 2013; Hoff & Bashir, 2015). How well these capabilities are conveyed to the operator is essential (Lee & See, 2004). There are two general ways of conveying the system capabilities: through an explicit description of the capabilities (i.e., description), or through experiencing the system (i.e., experience). These two ways of conveying information have been studied widely in human decision-making literature (Wulff, Mergenthaler-Canseco, & Hertwig, 2018). Yet, there has not been systematic investigation on these different methods of conveying information in the applied area of human-automation interaction (but see Chen, Mishler, Hu, Li, & Proctor, in press; Mishler et al., 2017).Furthermore, trust and reliance on automation is not only affected by the reliability of the automation, but also by the error types, false alarms and misses (Chancey, Bliss, Yamani, & Handley, 2017; Dixon & Wickens, 2006). False alarms and misses affect human performance in qualitatively different ways, with more serious damage being caused by false-alarmprone automation than by miss-prone automation (Dixon, Wickens, & Chang, 2004). In addition, false-alarm-prone automation reduces compliance (i.e., the operator’s reaction when the automation presents a warning); and miss-prone automation reduces reliance (i.e., the operator’s inaction when the automation remains silent; Chancey et al., 2017).Current StudyThe goal of the current study was to examine how the methods of conveying system reliability and automation error type affect human decision making and trust in automation. The automation system was a phishing-detection system, which provided recommendations to users as to whether an email was legitimate or phishing. The automation reliability was defined as the percentage of correct recommendations (60% vs. 90%). For each reliability level, there were a false-alarm condition, with all the automation errors being false alarms, and a miss condition, with all the errors being misses. The system reliability was conveyed through description (with an exact percentage described to the user) or experience (with immediate feedback to help the user learn; Barron, & Erev, 2003).A total of 510 participants were recruited and completed the experiment online through Amazon Mechanical Turk. The experimental task consisted of classifying 20 emails as phishing and legitimate, with a phishing-detection system providing recommendations. At the end of the experiment, participants rated their trust in this automated aid system. The measures included a performance measure (the decision accuracy made by the participants), as well as two trust measures (participants’ agreement rate with the phishing-detection system, and their self-reported trust in the system).Our results showed that higher system reliability and feedback increased accuracy significantly, but description or error type alone did not affect accuracy. In terms of the trust measures, false alarms led to lower agreement rates than did misses. With a less reliable system, though, the misses caused a problem of inappropriately higher agreement rates; this problem was reduced when feedback was provided for the unreliable system, indicating a trust-calibration role of feedback. Self-reported trust showed similar result patterns to agreement rates. Performance was improved with higher system reliability, feedback, and explicit description. Design implications of the results included that (1) both feedback and description of the system reliability should be presented in the interface of an automation aid whenever possible, provided that the aid is reliable, and (2) for systems that are unreliable, false alarms are more desirable than misses, if one has to choose between the two.</details>

***

**_Evaluating effects of automation reliability and reliability information on trust, dependence and dual-task performance_**  
Du, Na; Zhang, Qiaoning; Yang, X. Jessie  
https://doi.org/10.1177/1541931218621041  
<details><summary>Abstract</summary>The use of automated decision aids could reduce human exposure to dangers and enable human workers to perform more challenging tasks. However, automation is problematic when people fail to trust and depend on it appropriately.Existing studies have shown that system design that provides users with likelihood information including automation certainty, reliability, and confidence could facilitate trust- reliability calibration, the correspondence between a person’s trust in the automation and the automation’s capabilities (Lee & Moray, 1994), and improve human–automation task performance (Beller et al., 2013; Wang, Jamieson, & Hollands, 2009; McGuirl & Sarter, 2006).While revealing reliability information has been proposed as a design solution, the concrete effects of such information disclosure still vary (Wang et al., 2009; Fletcher et al., 2017; Walliser et al., 2016). Clear guidelines that would allow display designers to choose the most effective reliability information to facilitate human decision performance and trust calibration do not appear to exist. The present study, therefore, aimed to reconcile existing literature by investigating if and how different methods of calculating reliability information affect their effectiveness at different automation reliability.A human subject experiment was conducted with 60 participants. Each participant performed a compensatory tracking task and a threat detection task simultaneously with the help of an imperfect automated threat detector. The experiment adopted a 2×4 mixed design with two independent variables: automation reliability (68% vs. 90%) as a within- subject factor and reliability information as a between-subjects factor. Reliability information of the automated threat detector was calculated using different methods based on the signal detection theory and conditional probability formula of Bayes’ Theorem (H: hits; CR: correct rejections, FA: false alarms; M: misses):Overall reliability = P (H + CR | H + FA + M + CR).Positive predictive value = P (H | H + FA); negative predictive value = P (CR | CR + M).Hit rate = P (H | H + M), correct rejection rate = P (CR | CR + FA).There was also a control condition where participants were not informed of any reliability information but only told the alerts from the automated threat detector may or may not be correct. The dependent variables of interest were participants’ subjective trust in automation and objective measures of their display-switching behaviors.The results of this study showed that as the automated threat detector became more reliable, participants’ trust in anddependence on the threat detector increased significantly, and their detection performance improved. More importantly, there were significant differences in participants’ trust, dependence and dual-task performance when reliability information was calculated by different methods. Specifically, when overall reliability of the automated threat detector was 90%, revealing positive and negative predictive values of the automation significantly helped participants to calibrate their trust in and dependence on the detector, and led to the shortest reaction time for detection task. However, when overall reliability of the automated threat detector was 68%, positive and negative predictive values didn’t lead to significant difference in participants’ compliance on the detector. In addition, our result demonstrated that the disclosure of hit rate and correct rejection rate or overall reliability didn’t seem to aid human-automation team performance and trust-reliability calibration.An implication of the study is that users should be made aware of system reliability, especially of positive/negative predictive values, to engender appropriate trust in and dependence on the automation. This can be applied to the interface design of automated decision aids. Future studies should examine whether the positive and negative predictive values are still the most effective pieces of information for trust calibration when the criterion of the automated threat detector becomes liberal.</details>

***

**_Eye tracking: a promising method for inferring trust in real time_**  
Lu, Yidu; Sarter, Nadine  
https://doi.org/10.1177/1541931218621042  
<details><summary>Abstract</summary></details>

***

**_Automation reliability and trust: A Bayesian inference approach_**  
Wang, Chenlan; Zhang, Chongjie; Yang, X. Jessie  
https://doi.org/10.1177/1541931218621048  
<details><summary>Abstract</summary>Research shows that over repeated interactions with automation, human operators are able to learn how reliable the automation is and update their trust in automation. The goal of the present study is to investigate if this learning and inference process approximately follow the principle of Bayesian probabilistic inference. First, we applied Bayesian inference to estimate human operators’ perceived system reliability and found high correlations between the Bayesian estimates and the perceived reliability for the majority of the participants. We then correlated the Bayesian estimates with human operators’ reported trust and found moderate correlations for a large portion of the participants. Our results suggest that human operators’ learning and inference process for automation reliability can be approximated by Bayesian inference.</details>

***

**_Navigating the Advent of Human-Machine Teaming_**  
Christopher Brill, J.; Cummings, M. L.; Evans, A.w.; Hancock, Peter A.; Lyons, Joseph B.; Oden, Kevin  
https://doi.org/10.1177/1541931218621104  
<details><summary>Abstract</summary>The objective of this panel was to discuss issues related to human-machine (or human-agent) teaming (HMT). Panelists were selected to represent diverse interests and backgrounds (i.e., defense, industry, and academia). Chris Brill provided opening remarks to frame the discussion and introduce the panelists. He then raised several questions related to HMT, such as what is HMT, what level of autonomy is required for HMT, and how do we develop trust in autonomous teammates that learn, change, and potentially, individuate. Missy Cummings built on the issue of learning systems, addressing challenges of certifying systems that, as a function of learning, may cease to be known quantities. Bill Evans spoke to the need for transparency in human-agent teaming. Joseph Lyons addressed social factors in HMT. Peter Hancock detailed his concerns about whether forays into HMT are even advisable, particularly as doing so may lead to dehumanization, or worse, volitional demotion of humans from our current status as apex lifeforms on Earth. Lastly, Kevin Oden expanded the discussion of trust in autonomous systems, while also providing thoughts on how to best leverage human capabilities in the context of HMT. The panel then turned to facilitated discussion with panelists and audience members, constituting the majority of the session time. The session concluded with panelists summarizing their thoughts on how HF/E professionals can or should play a role in the advent of HMT.</details>

***

**_Implications of Trust on Patient Generated Health Data_**  
Papautsky, Elizabeth Lerner; Panganiban, April Rose  
https://doi.org/10.1177/1541931218621125  
<details><summary>Abstract</summary>Propelled by a focus on patient-centered care, technology development is moving in a direction of providing the patient formal avenues of contributing health information. Specifically, patient generated health data (PGHD), ranging from biometric to health status interpretation is an emerging topic in health informatics (Hull, 2015). Further, tools are in development to allow for the electronic delivery of PGHD from the patient to the provider in the electronic health record (EHR).At its core, human factors is concerned with representing information in a way that supports safe and efficient decision making. PGHD is a topic area that requires a human factors perspective for multiple reasons including the need to identify user needs and requirements of data capture on the patient side, data use on the provider side, and supporting technology design to support these users effectively. Much research in health informatics and healthcare human factors is concerned with the EHR, as the system has not reached intended design, forcing clinicians to search through a broad and deep information space to make decisions (Sittig, Wright, Ash, & Singh, 2016). As an emerging concept in health informatics, PGHD will serve as an additional informational category within the EHR for providers to consider in medical decision making. Therefore, one construct that needs to be examined is trust.The topic of trust has considerable relevance as reflected by prolific research efforts in domains in which automated technology and artificial intelligence dominate. Understanding and consideration of trust factors has been integral in complex domains where technological systems carry out functions ranging from representing and delivering information, providing decision support, to carrying out complex tasks.Trust is defined by Mayer, David, & Schoorman as a “willingness to be vulnerable to the actions of another party based on the expectation that the other will perform a particular action important to the trustor, irrespective of the ability to monitor or control that other party” (Mayer, David & Schoorman, 1995, pg 5). In healthcare, examination of trust factors has been limited to trust in technology, from both provider and patient perspectives, and trust in the provider from the patient perspective (interpersonal trust) (Montague, 2010; Montague, Winchester, & Kleiner, 2010). Albeit limited, evidence does suggest that trust plays a role in healthcare outcomes. For instance, higher trust in one’s primary care physician can increase proactive health behaviors such as following up with recommended colorectal cancer screenings (Gupta, Brenner, Ratanawangsa, & Inadomi, 2014).However, as of yet, no literature exists examining trust factors associated with the patient from the provider’s perspective either in face-to-face interactions or technology mediated. Therefore, there is a need to examine and consider trust to inform the design and implementation of electronic PGHD systems addressing the needs of both providers and patients in the capture and delivery of relevant content that may be useful and usable to the provider. Leveraging the knowledge from the trust in automation literature (Parasuraman, Sheridan, & Wickens, 2000), we can begin to examine issues of reliance and trust of PGHD.</details>

***

**_Teaming with Technology at the TSA: An Examination of Trust in Automation’s Influence on Human Performance in Operational Environments_**  
Korbelak, Kristopher; Dressel, Jeffrey; Tweedie, Donald; Wilson, Whitney; Erchov, Simone; Hilburn, Brian  
https://doi.org/10.1177/1541931218621150  
<details><summary>Abstract</summary>Automated systems are not only commonplace but a necessity to complete highly specialized tasks in many operational environments. Problems arise, however, when the automation is used injudiciously. Trust is known to influence how workers use and rely on automated systems, especially when the operational environment poses a great amount of complexity for the user. The environment in which most Transportation Security Administration (TSA) workers operate is characterized by complexity that often demands the use of automation to complete required tasks. The TSA aims to better understand the influence of trust in automation on operational performance to better support its mission and workforce. This paper will discuss the methods, findings, and practical implications gleaned from an examination of the role trust plays on human-automation interactions in the operational environment at TSA.</details>

***

**_Trust and Approachability Mediate Social Decision Making in Human-Robot Interaction_**  
Tulk, Stephanie; Wiese, Eva  
https://doi.org/10.1177/1541931218621160  
<details><summary>Abstract</summary>As humanoid robots become more advanced and commonplace, the average user may perceive their robotic companion as human-like entities that can make social decisions, such as the deliberate choice to act fairly or selfishly. It is important for scientists and designers to consider how this will affect our interactions with social robots. The current paper explores how social decision making with humanoid robots changes as the degree of their human-likeness changes. For that purpose, we created a spectrum of human-like agents via morphing that ranged from very robot-like to very human-like in physical appearance (i.e., in increments of 20%) and measured how this change in physical humanness affected decision-making in two economic games: the Ultimatum Game (Experiment 1) and Trust Game (Experiment 2). We expected increases in human-like appearance to lead to higher rates of punishment for unfair offers and higher ratings of trust in both games. While physical humanness did not have an impact on economic decisions in either of the ex-periments, follow-up analyses showed that both subjective ratings of trust and agent approachability medi-ated the effect of agent appearance on decision-making in both experiments. Possible consequences of these findings for human-robot interactions are discussed.</details>

***

**_Have a Heart: Predictability of Trust in an Autonomous Agent Teammate through Team-Level Measures of Heart Rate Synchrony and Arousal_**  
Tolston, Michael T.; Funke, Gregory J.; Alarcon, Gene M.; Miller, Brent; Bowers, Margaret A.; Gruenwald, Christina; Capiola, August  
https://doi.org/10.1177/1541931218621162  
<details><summary>Abstract</summary>Progression toward sophisticated machines with the capacity to act as partners in tactical and strategic situations means that human operators will increasingly rely on collaborative input from agent teammates (e.g., Masiello, 2013). However, plans to team autonomous agents with humans raise new questions regarding the effects that such teammates might have on important team psychological processes, such as team cognition and trust. Specifically, it is not known how modifications in team structure, such as changes in team size, influence team dynamics and psychological processes when the team includes an artificial agent, nor how trust established in such teams transfers to new environments, nor how measures that have been used to predict trust in humans generalize to agent teammates. Our current research examined these effects through the detection and analysis of an objective team phenomenon known as physio-behavioral coupling (PBC) using Multidimensional Recurrence Quantification Analysis (MdRQA; Wallot, Roepstorff, and Mønster, 2016) of shared physiological arousal during initial team formation and training. In particular, as shared physiological arousal measured in changing heart rhythms within human teams has been shown to be associated with measures of trust (Mitkidis, McGraw, Roepstorff, & Wallot, 2015) and concern for others (Konvalinka et al., 2011), we investigated how shared physiological arousal predicts willingness to trust an agent teammate in a novel task environment.We conducted an experiment consisting of collecting physio-behavioral data (i.e., heart rate) from teams of different sizes as they performed a series of collaborative, consensus building tasks. The independent variable was team size (teams of 2 or 3 human players, with an artificial agent teammate always present), and there were two separate team-oriented tasks: A first-round consensus-building wagering task, and a second-round task in which teams were able to make wagers on the expected performance of the agent teammate in a subsequent maze running task called Checkmate (Alarcon et al., 2017). We predicted that complimentary combinations of PBC (e.g., measures of overall similarity and stability in heart rate dynamics) obtained from MdRQA, along with self-reported measures of team and agent trust, would be positively related to future trusting behaviors in the agent teammate, and that increasing the number of teammates would result in higher order, more complex structure in the physio- behavioral data that would not be reducible to simpler patterns (e.g., Wallot et al., 2016). To this end, we predicted that measures of self-reported trust and multivariate PBC would be reducible to meaningful lower dimensional structures using principal components analysis (PCA), and that PBC calculated from the first task from the full team, but not from averages aggregated from subsets of the team, would significantly predict trusting behavior in the second task.Ninety-two participants (31 men and 61 women) recruited from the campus of a midwestern university in theU.S. took part in this study (19 dyads and 18 triads). Ages ranged from 18 to 42 (M = 22, SD = 5.48). The experiment was a univariate (team size; two or three human teammates with an agent teammate always present) between-subjects design. Self-reported measures were collected from each team member before each of the two tasks and included items that measured: Team ability, team benevolence, team integrity, and team trust (adapted from Mayer & Davis, 1999); trust in human teammates (adapted from Naquin & Paulson, 2003); agent competence, cognitive trust in the agent, emotional trust in the agent, intention to delegate to the agent, and intention to adopt the agent as an aid (adapted from Komiak & Benbasat, 2006); and collective efficacy (adapted from Riggs & Knight, 1994).Factor analysis of the composite scales from aggregated survey data indicated the data loaded well onto factors that corresponded to trust in the team and trust in the agent teammate. Factor analysis of MdRQA from the full team and from the averaged lower order analyses showed that each had one component with an eigenvalue greater than what would be expected by chance. Results from analyses using logistic regression to predict Checkmate betting showed that self-reported measures of trust in the agent and MdRQA of full team PBC in the initial task significantly predicted subsequent trusting behavior in an agent teammate in Checkmate, but lower-order PBC estimated from averages of team subgroups did not. These results suggest that multivariate team-level coupling has predictive power in subsequent team outcomes that cannot be fully captured using data aggregated from subgroup averages, and that measures of PBC measured from human teammates is related to trust in an agent teammate.We note two important contributions of the present study. First, that PBC and subjective measures of trust were significant predictors of observed trusting behavior regardless of team size suggests that important team processesand outcomes are at least partially invariant to changes in team size, a promising outcome for the prospect of meaningfully scaling measures of PBC beyond the typical dyadic context. Second, we have shown that shared team- level arousal is a significant predictor of subsequent trusting behavior in an agent teammate in a novel task, demonstrating that these objective measures are extensible to trust in non-human partners.</details>

***

**_Cue Utilization, Perceptions, and Experience in the Interpretation of Weather Radar Returns_**  
Wiggins, Mark W.; Crane, Monique; Loveday, Thomas  
https://doi.org/10.1177/1541931218621164  
<details><summary>Abstract</summary>This study was designed to examine the role of cue utilization, perceptions, and measures of operational experience in the interpretation of a scenario involving the interpretation of weather radar returns. A total of 47 qualified pilots completed EXPERTise 2.0, an online assessment of cue utilization in the context of weather radar systems. They also completed a scenario involving the interpretation of weather radar returns which required an assessment as to whether they could continue the flight safely in the absence of a change in track or altitude. Consistent with research in other domains, the results revealed a relationship between performance and cue utilization. No relationships were evident on the basis of flight experience nor the inclination to use or trust weather radar systems. The results provide the basis for a tool that might be employed to assess pilots’ cue utilization, thereby enabling more targeted approaches to pilot training and weather radar system design.</details>

***

**_Stereotypical of Us to Stereotype Them: The Effect of System-Wide Trust on Heterogeneous Populations of Unmanned Autonomous Vehicles_**  
Kluck, Molly; Kohn, Spencer C.; Walliser, James C.; de Visser, Ewart J.; Shaw, Tyler H.  
https://doi.org/10.1177/1541931218621253  
<details><summary>Abstract</summary>Operators generalize their trust across all of the autonomous agents they are working with, a phenomenon referred to as System Wide Trust (SWT). As a result, the failure of one aid can cause a trust decrement in —and therefore disuse of— all other competent aids within the system. This study explored two possible SWT mitigation strategies: competence transparency and different appearance of aids. Previous research has shown that transparency and feedback affects trust calibration in systems (Walliser et al., 2016), yet our appearance manipulation is relatively novel, using gestalt principles of grouping to explore whether heterogeneous aids will be more easily differentiated compared to homogenous aids. Participants supervised four UAVs that identified targets as enemies or friendlies. Only one of these UAVs was inaccurate (70% recommended accuracy), which caused a SWT trust decrement for all UAVs. We expected that the heterogeneous UAVs with competence transparency would suffer the least SWT effect, yet the results did not find a difference between conditions. These findings suggest that the System Wide Trust effect is too strong to be affected by our manipulations and that further research on mitigation strategies is required.</details>

***

**_Trust Repair Strategies with Self-Driving Vehicles: An Exploratory Study_**  
Kohn, Spencer C.; Quinn, Daniel; Pak, Richard; de Visser, Ewart J.; Shaw, Tyler H.  
https://doi.org/10.1177/1541931218621254  
<details><summary>Abstract</summary>Trust is important for any relationship, especially so with self-driving vehicles: passengers must trust these vehicles with their life. Given the criticality of maintaining passenger’s trust, yet the dearth of self-driving trust repair research relative to the growth of the self-driving industry, we conducted two studies to better understand how people view errors committed by self-driving cars, as well as what types of trust repair efforts may be viable for use by self-driving cars. Experiment 1 manipulated error type and driver types to determine whether driver type (human versus self-driving) affected how participants assessed errors. Results indicate that errors committed by both driver types are not assessed differently. Given the similarity, experiment 2 focused on self-driving cars, using a wide variety of trust repair efforts to confirm human-human research and determine which repairs were most effective at mitigating the effect of violations on trust. We confirmed the pattern of trust repairs in human-human research, and found that some apologies were more effective at repairing trust than some denials. These findings help focus future research, while providing broad guidance as to potential methods for approaching trust repair with self-driving cars.</details>

***

**_Effects of Automation Type on Human Performance in Proofreading Tasks_**  
Jeong, Heejin; Park, Jangwoon; Park, Jaehyun; Lee, Byung Cheol  
https://doi.org/10.1177/1541931218621261  
<details><summary>Abstract</summary>Automation is ubiquitous and indispensable in modern working environments. It is adopted and used in not only advanced industrial- and technology-oriented operations, but also ordinary home or office computational functions. In general, automated systems aim to improve overall work efficiency and productivity of labor-intensive tasks by decreasing the risk of errors, and cognitive and physical workloads. The systems offer the support for diverse decision-making processes as well. However, the benefits of automation are not consistently achieved and depend on the types and features of automation (Onnasch, Wickens, Li, & Manzey, 2014; Parasuraman, Sheridan, & Wickens, 2000). Possible negative side effects have been reported. Sometimes, automation may lead to multi-tasking environments, which allows operators to be distractive with several tasks. It ultimately prolongs task completion time and causes to neglect monitoring and follow-up steps of the pre-processing tasks (Endsley, 1996). Furthermore, the operators who excessively depend on automation are easily deteriorated in skill acquisition, which is necessary for the emergency or manual operations. Thus, inconsistent performance in automation is a major issue in successful adoption and trust in automation (Jeong, Park, Park, & Lee, 2017). This paper presents an experimental study that investigates the main features and causes of the inconsistency in task performance in different types of automation. Automated proofreading tasks were used in this study, which is one of the most common types of automation we experience in daily life. Based on the similar algorithm of the auto-correct function in Microsoft Word, a custom-built program of five proofreading tasks, including one non-automated and four automated proofreading tasks, were developed using Visual Studio 2015 C#. In the non-automated task used as a reference for individual difference, participants were asked to manually find a typographical error in a sentence. In the automated tasks, auto-correcting functions are provided in two levels (i.e., low and high) of automation and two statuses (i.e., routine and failure of automation). The type of automation is defined as the combinations of a status and a level. Participants identified typographical errors by only an underlined word at the low-level automation, whereas an underlined word with a possible substituting word was given at the high-level. Additionally, in the routine automation status, a correct substituting word is provided. On the other hand, a grammatically incorrect word is given in the failed automation status. Nineteen participants (11 females and 8 males; age mean = 33.8, standard deviation = 19.1) took part in this study. Results of statistical analyses show a clear advantage in high-routine automation, in terms of both task completion time and accuracy. While task performances of high & routine automation types are quite obvious in both task completion time and accuracy, those in the failed automation types are mixed and indistinguishable. Different levels and statues of failed automation do not much influence task performance. Moreover, task completion time and mental demand are strongly correlated, and the accuracy rate and perceived trust show a strong positive correlation. The approaches and outcomes of the current study can provide some insights into the human-automation interaction systems that support human performance and safety, such as in-vehicle warning systems and automated vehicle controls.</details>

***

**_Effects of Automation Reliability and Trust on System Monitoring Performance in Simulated Flight Tasks_**  
Ferraro, James; Clark, Logan; Christy, Naomi; Mouloua, Mustapha  
https://doi.org/10.1177/1541931218621283  
<details><summary>Abstract</summary>The current study was designed to empirically examine the effects of trust and automation reliability on multi-tasking performance in a simulated cockpit setting using the Multi-Attribute Task Battery II (MATB-II). The MATB-II simulates tasks often performed by pilots in-flight, tasking the operator with attending to automated systems and correcting errors when they inevitably occur. Over the course of three 30-minute trials, two levels of automation reliability were presented in the current study (R50% and R90%). It was hypothesized that automation reliability and trust would affect both workload and performance in this multi-tasking environment. Results indicated that reliability significantly affected monitoring performance on the MATB-II. More reliable automation resulted in poorer monitoring performance, while trust appeared to have little impact. These results provide further evidence for how operators trust and interact with automation, a topic that is relevant to the implementation of automated systems in a variety of human-machine systems such as aviation.</details>

***

**_Warnings for Hurricane Irma: Trust of Warning Type and Perceptions of Self-Efficacy and Susceptibility_**  
Parker, Jason A.; Whitmer, Daphne E.; Sims, Valerie K.  
https://doi.org/10.1177/1541931218621312  
<details><summary>Abstract</summary>The purpose of this study was to examine the types of risk communication received about Hurricane Irma by a university sample, along with their perceptions of self-efficacy and susceptibility to the storm. Three days after the storm, 176 individuals completed a survey that asked about how they received alerts, the frequency of the alerts received, and their trust in the different risk communication mediums. Additionally, respondents completed a susceptibility measure, a self-efficacy measure, and a storm fear questionnaire. Results showed that most people received alerts from their university alert system or social media. Participants trusted risk communication the most from text alerts and radio reports, but the least from social media. Additionally, results showed that those who received more alerts also had higher levels of perceived susceptibility to the hurricane, except for those who received 16 to 20 alerts. Perceived self-efficacy was not related to the number of alerts received. These data suggest that although many urge the use of social media for spreading emergency warnings, people distrust social media for risk communication, and that this mistrust may be due to recent cases of misinformation spreading on various platforms. In addition, these data suggest that there may be a “critical point” of alerting, such that receiving more than 5 hurricane alerts may lead to significant increase in perceptions of susceptibility to the storm. Future research should investigate the critical point of effective alerting and the effect that trust in the different mediums of alert technology has on motivation to comply with the warning’s protective action recommendations.</details>

***

**_Understanding Attitudes Towards Self-Driving Vehicles: Quantitative Analysis of Qualitative Data_**  
Lee, John D.; Kolodge, Kristin  
https://doi.org/10.1177/1541931218621319  
<details><summary>Abstract</summary>Self-driving vehicles represent potentially transformative technology. But achieving this potential depends on people’s attitudes towards this technology and willingness to use it. Ratings from surveys estimate acceptance, and open-ended comments provide an opportunity to understand the “why” behind the ratings. One way to understand the content of open-ended comments is through computer-based text analytics. A recent survey of 8,571 nationally representative drivers in J.D. Power’s 2017 U.S. Tech Choice StudySM included a rating of willingness to use self-driving vehicles and an associated open-ended response. We present a quantitative analysis of these qualitative, open-ended responses that uses structural topic modeling to reveal the basis of the respondents’ attitudes. Drivers’ attitude towards self-driving vehicles was quite negative: only 11% stated they “definitely would” trust self-driving technology, whereas 35% stated they “definitely would not.” The structural topic modeling identified 10-topics, such as “Many unknowns” and “Don’t trust” that help explain these negative attitudes.</details>

***

**_Trust Strategies in Consumer Multiple-Component Systems_**  
Lopez, Jeremy; Pak, Richard  
https://doi.org/10.1177/1541931218621397  
<details><summary>Abstract</summary>Trust is a critical factor in successful and productive human-automation interactions. Human trust in machines is sensitive to machine performance. When automation malfunctions, trust is negatively affected. The development of increasingly complex multiple-component systems, or those with several autonomous elements, introduces even more ways for a system to err. One example is in smart home control systems where different subsystems may be controlled by different autonomous routines or rules. Multiple studies suggest that one error-prone component can lower user trust in the remaining components. This has been termed a “pull down effect.” Other research suggests that increasing the amount of information presented to the user can reduce the strength of the pull down effect by promoting heterogeneity of components. Although a majority of this research has been tested in the industrial domain, there exist certain types of information that are best suited for consumer automation (e.g., granting the automation a name and a voice). Providing this kind of information to the user may diminish the strength of the pull down effect. Thus, the current study will investigate the effectiveness of trust-preserving heterogeneity strategies in consumer multiple-component systems.</details>

***

**_Trust in Branded Autonomous Vehicles & Performance Expectations: A Theoretical Framework_**  
Celmer, Natalie; Branaghan, Russell; Chiou, Erin  
https://doi.org/10.1177/1541931218621398  
<details><summary>Abstract</summary>Future autonomous vehicle systems will be diverse in design and functionality because they will be produced by different brands. It is possible these brand differences yield different levels of trust in the automation, therefore different expectations for vehicle performance. Perceptions of system safety, trustworthiness, and performance are important because they help users determine how reliant they can be on the system. Based on a review of the literature, the system’s perceived intent, competence, method, and history could be differentiating factors. Importantly, these perceptions are based on both the automated technology and the brand’s personality. The following theoretical framework reflects a Human Systems Engineering approach to consider how brand differences impact perceived trustworthiness, performance expectations and ultimate safety of autonomous vehicles.</details>

***

**_Revisiting Trust in Machines: Examining Human–Machine Trust Using a Reprogrammed Pasteurizer Task_**  
Lee, Jieun; Yamani, Yusuke; Itoh, Makoto  
https://doi.org/10.1177/1541931218621400  
<details><summary>Abstract</summary>Automated technologies have brought a number of benefits to professional domains, expanding the area in which humans can perform optimally in complex work environments. Human–automation trust has become an important aspect when designing acceptable automated systems considering general users who have no comprehensive knowledge of the systems. Muir and Moray (1996) proposed a model of human–machine trust incorporating predictability, dependability, and faith as predictors of overall trust in machines. Though Muir and Moray (1996) predicted that trust in machines grows from predictability, then dependability, and finally faith, their results suggested the opposite. This study will reexamine their theoretical framework and test which of the three dimensions governs initial trust in automation. Participants will be trained to operate a simulated pasteurization plant, as in Muir and Moray (1996), and they will be asked to maximize system performance in the pasteurizing task. We hypothesized that faith governs overall trust early in the interaction with the automated system, then dependability, and finally predictability as lay automation users become more familiar with the system. We attempt to replicate the results of Muir and Moray (1996) and argue that their model should be revised for trust development for general automation users.</details>

***

**_A Preliminary Study to Investigate the Sensemaking Process of UAV Reports by Operators after Periods of Disconnect for Threat Assessment_**  
Rogers, Hunter; Al Ghalayini, Maher; Madathil, Kapil Chalil  
https://doi.org/10.1177/1541931218621407  
<details><summary>Abstract</summary>Teleoperated robots have been used to minimize human presence and interaction in dangerous or difficult areas or to enhance human capabilities. One of the challenges that still needs investigation is human-robot communication regarding making decisions based on extracted information. As such, the purpose of this research is to address issues of decision making after communication disconnections through interface design by focusing on improving sensemaking. Analyzing the effects of interface design in a UAV threat assessment scenario is ideal to examine not only the sensemaking process but also how that process defines decision making in an environment where efficient decision making and low clutter displays are needed. The data from this study will serve determine elements of an optimal interface for information assimilation. Twenty participants were recruited to complete a series of sixteen threat assessment decisions with different interface designs, varying amounts of information and layouts of information. It was observed that level of detailed information had a significant effect on the subjective trust and decision selected and display layout had a significant effect on the time taken to complete a decision.</details>

***

**_Characterizing Driver Trust in Vehicle Control Algorithm Parameters_**  
Domeyer, Joshua; Venkatraman, Vindhya; Price, Morgan; Lee, John D  
https://doi.org/10.1177/1541931218621413  
<details><summary>Abstract</summary>Human factors research in vehicle automation has focused on user interfaces such as performance feedback through visual and auditory displays (Blanco et al., 2015). Another approach is to use vehicle dynamics and vibrations as communicative tools for guiding attention (e.g., Morando, Victor, & Dozza, 2016; Walker, Stanton, & Young, 2006; Wiese & Lee, 2007). In our previous study (Price, Venkatraman, Gibson, Lee, & Mutlu, 2016), we showed that the steering wheel deadband, or lateral movement of the vehicle while maintaining lane position, was negatively associated with trust—more lateral movement led to less trust in the algorithm. The present study extends these findings by using Bayesian statistical methods with new control algorithm data. Although the inclusion of additional algorithm characteristics did not improve the trust model, the use of Bayesian statistical methods provides a useful tool to incorporate prior knowledge into an analysis.</details>

***

**_Effects of Warning Characteristics on Driver Performance in Connected Vehicle Systems with Missing Warnings_**  
Zhang, Yiqi; Wu, Changxu; Qiao, Chunming; Hou, Yunfei  
https://doi.org/10.1177/1541931218621415  
<details><summary>Abstract</summary>The connected vehicle systems (CVS) aim to provide drivers with information in a timely and reliable way to improve transportation safety. With the emerging wireless communication technologies, the vehicles will be equipped with the ability to communicate with each other about the surrounding traffic situations by exchanging vehicle status and motion data via Dedicated Short-Range Communications (DSRC) network (Kenney, 2011).With the assistance of the cooperative collision warnings, the impact of designed warning parameters on driver performance is increasingly important. Existing empirical studies have studied the warning timing and warning reliability in determining the effectiveness of the collision warning systems in advanced driver assistance systems (ADAS). In terms of warning timing, the studies in reached consistent conclusions that early warnings induced more timely braking and longer braking process, resulted in higher trust of the warning systems, and reduced collision rates (for example, Abe & Richardson, 2006a; Lee, McGehee, Brown, & Reyes, 2002; Yan, Xue, Ma, & Xu, 2014; Yan, Zhang, & Ma, 2015; Wan, Wu, and Zhang, 2016). In terms of the warning reliability, research has shown that warnings with a higher reliability increased driver’s trust of the warning systems, led to higher frequency in warning responses, and reduced crash rates (for example, Abe, Itoh, & Yamamura, 2009; Bliss & Acton, 2003; Maltz & Shinar, 2007; Sullivan, Tsimhoni, & Bogard, 2008). However, the interaction effects of warning lead time and warning reliability on driver performance was not examined especially under the connected vehicle settings.The current research investigated the interaction effects of warning lead time (2.5s vs. 4.5s), warning reliability (73% vs. 89%), and speech warning style (command vs. notification) on driver performance and subjective evaluation of warnings in CVS. A driving simulator study with thirty-two participants was conducted to simulate a connected vehicle environment with missing warnings due to the failures in the data transmission within the communication network of the CVS.The results showed command warnings led to a smaller collision rate compared to notification warnings with the warning lead time of 2.5s, whereas notification warnings resulted in a smaller collision rate compared to command warning with the warning lead time of 4.5s. These results suggested notification warnings should be selected when warning lead time is longer and warning reliability is higher, which resulted in higher safety benefits and higher subjective ratings. Command warnings could be selected when warning lead time is shorter since they led to more safety benefits.However, such selection has to be made with caution since command warnings may limit drivers’ response type and were perceived as less helpful than notification warnings.</details>

***

**_Drivers’ Perceptions of Functionality Implied by Terms Used to Describe Automation in Vehicles_**  
Nees, Michael A.  
https://doi.org/10.1177/1541931218621430  
<details><summary>Abstract</summary>The expectations induced by the labels used to describe vehicle automation are important to understand, because research has shown that expectations can affect trust in automation even before a person uses the system for the first time. An online sample of drivers rated the perceived division of driving responsibilities implied by common terms used to describe automation. Ratings of 13 terms were made on a scale from 1 (“human driver is entirely responsible”) to 7 (“vehicle is entirely responsible”) for three driving tasks (steering, accelerating/braking, and monitoring). In several instances, the functionality implied by automation terms did not match the technical definitions of the terms and/or the actual capabilities of the automated vehicle functions currently described by the terms. These exploratory findings may spur and guide future research on this under-examined topic.</details>

***

**_Investigating Attorney Trust in Machine-enabled Legal Research: A Mixed Methods Approach_**  
Lowry, Katherine M; Kamp, Elaine; Fallon, Corey K; McGhee, Riley  
https://doi.org/10.1177/1541931218621452  
<details><summary>Abstract</summary>ROSS is a new legal research tool leveraging artificial intelligence. This tool is somewhat unique both in terms of its interface and underlying functionality. ROSS’s deviation from more traditional systems may have an impact on the adoption of this new technology. The researchers in the current study conducted a mixed methods evaluation of ROSS to assess user experience and likelihood of adoption with a particular emphasis on evaluating user trust in the technology. The researchers conducted simulation interviews with eleven experienced bankruptcy attorneys. In addition, a human-computer trust survey was administered to assess user trust in the technology at various time points throughout the study. This quantitative survey measured both cognitive and affect-based trust and revealed a substantial increase in trust with exposure to the system. Participants were particularly impressed with attributes of the system that are believed to form the cognitive basis of trust, such as the tool’s reliability and understandability. Despite these positive findings, our qualitative assessment of experiential trust was mixed and most users concluded that they would not use ROSS as their primary research tool. This decision may be attributed to a lack of faith in the technology and concerns related to ROSS’s usability. The simulations were conducted between February and April of 2017. It should be noted that ROSS has been updated since this time and certain findings may no longer be representative of the current platform.</details>

***

**_Pedestrians receptivity in autonomous vehicles: Exploring a video-based assessment_**  
Deb, Shuchisnigdha; Hudson, Christopher R.; Carruth, Daniel W.; Frey, Darren  
https://doi.org/10.1177/1541931218621465  
<details><summary>Abstract</summary>Pedestrian receptivity toward autonomous vehicles (AVs) usually depends on the extent to which they receive indication of the vehicle’s intended action. Previous studies have typically used overt subjective measures (trust measures, ratings, etc.) and few objective measures (walking speed, waiting time, etc.) to identify external features that can improve pedestrians’ receptivity toward AVs. The current study aims to evaluate pedestrians’ behavioral measures of receptivity based on their body (head and foot) movements as they experience an AV in a virtual traffic environment. Videos of pedestrians at a virtual crosswalk, interacting with an AV that was equipped with an external feature indicating different operator statuses were coded. The operator statuses used in this study included: no driver, attentive driver, and distracted driver. The external features used were: no feature, upraised hand, stop sign, walking silhouette, walk in text, music, and a verbal message. Pedestrian body movements were derived from the video to determine frequency for looking at the approaching vehicle while crossing and stops after initiating crossing. Average durations for initiating crossing after signal were calculated. For no feature condition, the waiting time was calculated when participants observed the car. Data were compared with pedestrians’ self-reported ratings for receptivity to investigate body movements’ sensitivity to participants’ receptivity level. Results suggest body movements are sensitive to individual differences in reported receptivity. Future work should further examine the utility of this behavioral metric by further examining situational differences.</details>

***

