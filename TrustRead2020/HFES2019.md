## [HFES2019](HFES2019.md) 63rd International Annual Meeting of the Human Factors and Ergonomics Society
Link: [[Proceedings](https://journals.sagepub.com/toc/proe/63/1)]
[[Conference](http://www.hfes2019.org/)]
Awardなし  
***
**_Trust in Automation as a Function of Transparency and Teaming_**  
  
https://doi.org/10.1177/1071181319631212  
<details><summary>Abstract</summary>Information analysis is a critical piece of successful operations. As automation becomes a more common element in this analysis, it is important to understand what information should be communicated to users to promote proper use of automation. Both transparency, information regarding how and why automation functions proceed, and uncertainty may be two vital pieces of communication. Method: Twenty-six participants completed a simulated tank identification task with an automated or human teammate across 3 levels of teammate transparency. Trials varied in presence or absence of uncertainty information. Trust and performance effects were evaluated. Users were shown a terrain image and had to determine if a military tank was present in the terrain. The teammate provided a Tank Present or Absent recommendation along with transparent reasoning for the recommendation. Participants then made a tank presence judgement. Results: Transparency increased user trust as well as reaction times. Having uncertainty information increased user trust, but decreased performance accuracy. Teammate type had no significant effect on trust, accuracy, or reaction time. Discussion: The data indicate that transparency and uncertainty information can help users form an accurate mental model of teammate capabilities.</details>

***

**_Neural Correlates of Trust During an Automated System Monitoring Task: Preliminary Results of an Effective Connectivity Study_**  
Sanders, Nathan; Choo, Sanghyun; Kim, Nayoung; Nam, Chang S.; Fitts, Edward P.  
https://doi.org/10.1177/1071181319631409  
<details><summary>Abstract</summary>As autonomous systems become more prevalent and their inner workings become more opaque, we increasingly rely on trust to guide our interactions with them especially in complex or rapidly evolving situations. When our expectations of what automation is capable of do not match reality, the consequences can be sub-optimal to say the least. The degree to which our trust reflects actual capability is known as trust calibration. One of the approaches to studying this is neuroergonomics. By understanding the neural mechanisms involved in human-machine trust, we can design systems which promote trust calibration and possibly measure trust in real time. Our study used the Multi Attribute Task Battery to investigate neural correlates of trust in automation. We used EEG to record brain activity of participants as they watched four algorithms of varying reliability perform the SYSMON subtask on the MATB. Subjects reported their subjective trust level after each round. We subsequently conducted an effective connectivity analysis and identified the cingulate cortex as a node, and its asymmetry ratio and incoming information flow as possible indices of trust calibration. We hope our study will inform future work involving decision-making and real-time cognitive state detection.</details>

***

**_Detecting Human Trust Calibration in Automation: A Deep Learning Approach_**  
Choo, Sanghyun; Sanders, Nathan; Kim, Nayoung; Kim, Wonjoon; Nam, Chang S.; Fitts, Edward P.  
https://doi.org/10.1177/1071181319631298  
<details><summary>Abstract</summary></details>

***

**_An Empirical Exploration of Resilience in Human-Autonomy Teams Operating Remotely Piloted Aircraft Systems_**  
Demir, Mustafa; McNeese, Nathan J.; Cooke, Nancy J.; Grimm, David A.; Gorman, Jamie C.  
https://doi.org/10.1177/1071181319631020  
<details><summary>Abstract</summary>Project overviewTeam resilience is an interactive and dynamic process that develops over time while a team maintains performance. This study aims to empirically investigate systems-level resilience in a Remotely Piloted Aircraft (RPA) System simulated task environment by examining team interaction during novel events. The approach used in the current study to measure systems-level resilience was developed by Hoffman & Hancock (2017). In their conceptual study, resilience was considered a key feature of success in emerging complex sociotechnical systems; in our case, that is applied to Human-Autonomy Teams (HATs). Hoffman and Hancock conceptualized a resilience measure dynamically by means of several components, such as the time it took the system to recognize and characterize anomalies, and the time taken to specify and achieve new goals. In their framework, there were two main sub-events which expressed resilience via time-based measures, and upon which we designed ours in this study: (1) time taken to design a new process and (2) time required to implement it (Hoffman & Hancock, 2017).DesignIn this current research, there were three heterogeneous team members who used a text-based system to communicate and successfully photograph target waypoints: (1) navigator – provided information regarding a flight plan with speed and altitude restrictions of each waypoint; (2) pilot – controlled the RPA by adjusting its altitude and airspeed through negotiating with the photographer in order to take a good photo of the target waypoints; and (3) photographer – screened camera settings and sent feedback to the other team members regarding the status of target’s photograph. This study followed the Wizard of Oz paradigm wherein the navigator and photographer were seated together in one room and were told that the pilot was a synthetic agent. In actuality, the pilot was a well-trained experimenter who was working from a separate room. This ‘synthetic’ pilot used restricted vocabulary to simulate that of a computer. The main manipulations in this study consisted of three degraded conditions: (1) automation failure - role-level display failures while processing specific targets, (2) autonomy failure - autonomous agent behaved abnormally while processing specific targets (i.e., it provided misinformation to other team members or demonstrated incorrect actions), and (3) malicious cyber-attacks - the hijacking of the synthetic agent, which led to the synthetic agent providing false, detrimental information to the team about the RPA destination. Because the malicious cyber-attack only occurred once (during the final mission), we will focus on the automation and autonomy failures for this study. Each failure was imposed at a selected target waypoint and the teams had to find a solution in a limited amount of time. The time limit for each failure was related to the difficulty of the failure. Each failure was introduced at a pre-selected target waypoint for each team.MethodIn this experiment, there were 22 teams, with only two participants randomly assigned to the navigator and photographer roles for each team, because the pilot was a highly-trained experimenter. The current task was comprised of ten 40-minute missions in which teams needed to take as many “good” photos as possible of ground targets while avoiding alarms and rule violations. For this study, using the RPAS paradigm, we calculated two team resilience scores (1) time taken to design a new process and (2) time required to implement it (Hoffman & Hancock, 2017). For the calculations, we used the message sent time (in seconds) for each role to express resilience in terms of the proportion of total task time (2400 seconds). As an outcome measure, we used target processing efficiency as a coordination and time-based performance score, which was based on how quickly teams were able to take a good photo of each target.Results and discussionWe found that teams were more resilient during automation failures and progressed toward targets more successfully than during autonomy failures. We see three possible explanations for this: (1) automation failures were more explicit than autonomy failures, since at least one team member interacted with other teammates; (2) autonomy failures took more time for human teammates to identify the failure, because the autonomous agent’s abnormal behavior was not as straight forward; and 3) human teammates overtrusted to the autonomous agent and lack confidence in themselves and let the failure go on.AcknowledgementsThis research is supported by ONR Award N000141712382 (Program Managers: Marc Steinberg, Micah Clark). We also acknowledge the assistance of Steven M. Shope of Sandia Research Corporation, who integrated the synthetic agent and the testbed.</details>

***

**_Enhancing Transparency in Human-autonomy Teaming via the Option-centric Rationale Display_**  
Luo, Ruikun; Du, Na; Huang, Kevin Y.; Yang, X. Jessie  
https://doi.org/10.1177/1071181319631366  
<details><summary>Abstract</summary>Human-autonomy teaming is a major emphasis in the ongoing transformation of future work space wherein human agents and autonomous agents are expected to work as a team. While the increasing complexity in algorithms empowers autonomous systems, one major concern arises from the human factors perspective: Human agents have difficulty deciphering autonomy-generated solutions and increasingly perceive autonomy as a mysterious black box. The lack of transparency could lead to the lack of trust in autonomy and sub-optimal team performance (Chen and Barnes, 2014; Endsley, 2017; Lyons and Havig, 2014; de Visser et al., 2018; Yang et al., 2017).In response to this concern, researchers have investigated ways to enhance autonomy transparency. Existing human factors research on autonomy transparency has largely concentrated on conveying automation reliability or likelihood/(un)certainty information (Beller et al., 2013; McGuirl and Sarter, 2006; Wang et al., 2009; Neyedli et al., 2011). Providing explanations of automation’s behaviors is another way to increase transparency, which leads to higher performance and trust (Dzindolet et al., 2003; Mercado et al., 2016). Specifically, in the context of automated vehicles, studies have showed that informing the drivers of the reasons for the action of automated vehicles decreased drivers’ anxiety, increased their sense of control, preference and acceptance (Koo et al., 2014, 2016; Forster et al., 2017).However, the studies mentioned above largely focused on conveying simple likelihood information or used hand-drafted explanations, with only few exceptions (e.g.(Mercado et al., 2016)). Further research is needed to examine potential design structures of transparency autonomy.In the present study, we wish to propose an option-centric explanation approach, inspired by the research on design rationale. Design rationale is an area of design science focusing on the “representation for explicitly documenting the reasoning and argumentation that make sense of a specific artifact (MacLean et al., 1991)”. The theoretical underpinning for design rationale is that for designers what is important is not just the specific artifact itself but its other possibilities – why an artifact is designed in a particular way compared to how it might otherwise be. We aim to evaluate the effectiveness of the option-centric explanation approach on trust, dependence and team performance.We conducted a human-in-the-loop experiment with 34 participants (Age: Mean = 23.7 years, SD = 2.88 years). We developed a simulated game Treasure Hunter, where participants and an intelligent assistant worked together to uncover a map for treasures. The intelligent assistant’s ability, intent and decision-making rationale was conveyed in the option-centric rationale display. The experiment used a between-subject design with an independent variable – whether the option-centric rationale explanation was provided. The participants were randomly assigned to either of the two explanation conditions. Participants’ trust to the intelligent assistant, confidence of accomplishing the experiment without the intelligent assistant, and workload for the whole session were collected, as well as their scores for each map.The results showed that by conveying the intelligent assistant’s ability, intent and decision-making rationale in the option-centric rationale display, participants had higher task performance. With the display of all the options, participants had a better understanding and overview of the system. Therefore, they could utilize the intelligent assistant more appropriately and earned a higher score. It is notable that every participant only played 10 maps during the whole session. The advantages of option-centric rationale display might be more apparent if more rounds are played in the experiment session. Although not significant at the .05 level, there seems to be a trend suggesting lower levels of workload when the rationale explanation displayed.Our study contributes to the study of human-autonomy teaming by considering the important role of explanation display. It can help human operators build appropriate trust and improve the human-autonomy team performance.</details>

***

**_Effects of Anthropomorphic Phishing Detection Aids, Transparency Information, and Feedback on User Trust, Performance, and Aid Retention_**  
Mishler, Scott; Jeffcoat, Cody; Chen, Jing  
https://doi.org/10.1177/1071181319631351  
<details><summary>Abstract</summary>Phishing email attacks are a prevalent threat to internet users. Users often ignore or otherwise disregard automated aids, even when the aids’ reliability is high. The current study sought to fill a gap in the literature by examining the effects of anthropomorphism, feedback, and transparency information on user trust and performance within the domain of phishing email detection. Based upon previous studies in anthropomorphic automated systems, this study incorporated three levels of anthropomorphism (AI, human, text), two levels of aid gender (male, female), transparency information (present, absent), and feedback (present, absent). The 465 participants were recruited online through Amazon Mechanical Turk (MTurk) and performed the study on Qualtrics. Phishing was explained and instructions told the participants to judge whether the following emails are legitimate or phishing in three separate blocks of five emails. The first block was without any automated aid as a baseline of participants’ performance. The second block showed participants their respective aid and had them complete five more emails with the aid. The final block allowed participants to choose if they wanted to keep the aid or classify the emails alone. Afterwards, participants were asked how much they trusted the aid to help detect phishing threats using a trust in automation scale based on Jian, Bisantz, and Drury's (2000) study. Our results revealed improved performance on the phishing detection task for participants with an aid over participants without an aid. In addition, feedback was shown to be helpful for improving judgement accuracy as well as increase trust. Transparency also improved judgement accuracy for the human aid but was less helpful for the AI aid. This study compliments past research indicating improvements in performance with automated aids (Chen et al., 2018; Röttger, Bali, & Manzey, 2009; Wiegmann, Rich, & Zhang, 2001). Performance in blocks 2 and 3 was better than block 1. A significant positive correlation between trust and performance reinforces that high trust in a highly reliable aid begets good performance. Subsequently, if participants did not retain the aid for block 3, their performance was worse than those who retained the aid. Designers of automated aid systems should prioritize users interacting with and using the aid so that performance stays high. Feedback also helped improve judgement accuracy. By allowing participants to understand the reliability of the aid, they could learn to trust it more and rely on the suggestions of the aid. Feedback information should be offered to users if possible because it helps improve trust and performance, which is the goal of any automated aid system. Human aids with transparency information helped improve performance compared to human aids without transparency information. But this effect was not found for AI aids and nearly reversed. Transparency was expected to improve trust and performance (Hoff & Bashir, 2015), but it showed no differences in trust and only improved performance for human aids. This new finding demonstrates that there could be differences in the perception of human and AI aids, although further experiments would need to be conducted to further examine these findings.</details>

***

**_Generational differences in trust in digital assistants_**  
Noah, Benjamin; Sethumadhavan, Arathi  
https://doi.org/10.1177/1071181319631029  
<details><summary>Abstract</summary>Human trust in automation has been studied extensively within safety critical domains (military, aviation, process control, etc.) because harmful consequences are associated with the improper calibration of trust in automated systems in these domains (Parasuraman & Riley, 1997). As such, researchers have worked to identify important factors which help humans build trust in such systems (Hoff & Bashir, 2015). With the explosion of AI in consumer technologies, it is becoming equally critical to understand how humans interact with everyday devices. This study investigated how factors that have been identified to impact trust in automation in safety critical domains influence the trust and use of popular digital assistants (Siri, Cortana, Bixby or Google Now). We conducted an online survey with 278 regular users of digital assistants across three generations (GenX, GenY, and GenZ). The results demonstrate that, even after controlling for dispositional factors (i.e., individual characteristics such as age, culture, gender), GenZ exhibited higher trust in digital assistants than GenX. More interestingly, linear regression analyses revealed a different set of predictors of trust for each generation. Results from this survey have implications for the design of digital assistants.</details>

***

**_Attribution Biases and Trust Development in Physical Human-Machine Coordination: Blaming Yourself, Your Partner or an Unexpected Event_**  
Hsiung, Chi-Ping; Chiou, Erin  
https://doi.org/10.1177/1071181319631039  
<details><summary>Abstract</summary>Reading partners’ actions correctly is essential for successful coordination, but interpretation does not always reflect reality. Attribution biases, such as self-serving and correspondence biases, lead people to misinterpret their partners’ actions and falsely assign blame after a surprise, or unexpected event. These biases further influence people’s trust in their partners, including machine partners (Muir, 1987; Madhavan & Wiegmann, 2004). Advances in robotics have allowed for robots to partner with people at work and be treated socially (Young, Hawkins, Sharlin & Igarashi, 2009). However, these advances may interfere with a person’s appropriate calibration of trust in robots (Parasuraman & Miller, 2004). A better understanding of attribution biases in the wake of an unexpected event may shed light on how trust develops in a robot partner. This study was built on a human coordination example to serve as a reference for future human-robot interactions. We posit that attribution biases lead people to blame their partner after experiencing a negative performance outcome, thus lowering their trust in the partner.Sixty participants (30 pairs) were tasked to coordinate with an unfamiliar human partner, to lift a 17.5 lb. box containing a 200ml cup of water filled to the brim, from the floor to a table, as quickly as possible without spilling water. Before the task, participants were told that the pair with the best performance would be rewarded; however, all pairs were told they did not achieve this. Participant pairs were randomly assigned to a surprise condition during which they heard a 250 Hz warning tone, or a baseline condition with no warning tone. Participants in both conditions were told to pause the task as quickly as possible if the warning tone was present. It was unknown to participants when or if a warning tone would occur. To assess participants’ trust in their partner, Muir’s (1987) trust questionnaire was administered twice, once after introducing the task to participants, and again after the coordination task was completed. To capture blame assignment, a scale based on Kim and Hinds (2006) was administered after participants were told they did not achieve the best performance.Results indicate participants were less likely to blame their partners for the negative outcome, compared to blaming themselves or the warning tone itself (in the surprise condition). Next, surprisingly, in the surprise condition, instead of experiencing a decrease of trust in a partner after the negative outcome, there was a significant increase in trust in their partners. No significant difference in trust was found in the baseline condition. Finally, results also indicate that initial trust in a partner is a significant predictor for how people assign blame. In general, the effects of attribution biases were not observed in the present study. Friendliness may be a factor in people’s assignment of blame; although participants were unfamiliar with one another, all participants were students at the same university. Second, shared experience during the surprise condition, including the chance to assess their partner’s behaviors in response to the warning tone, may have been a catalyst for increased trust in a partner. It is important to note that although physical differences between participants were not evaluated in this study, height may be a potential confounding factor in this task. These findings enlighten our understanding of physical human-robot coordination scenarios and trust in a partner.</details>

***

**_Subjective Measurement of Trust: Is It on the Level?_**  
Wei, Jiajun; Bolton, Matthew L.; Humphrey, Laura  
https://doi.org/10.1177/1071181319631062  
<details><summary>Abstract</summary>Psychometrics are increasingly being used to evaluate trust in the automation of safety-critical systems. There is no consensus on what the highest level of measurement is for psychometric trust. This is important as the level of measurement determines what mathematics and statistics can be meaningfully applied to ratings. In this work, we introduce a new method for determining what the maximum level of measurement is for psychometric ratings. We use this to assess the level of measurement of trust in automation using human ratings about the behavior of unmanned aerial systems performing search tasks. Results show that trust is best represented at an ordinal level and that it can be treated as interval in most situations. It is unlikely that trust in automation ratings are ratio. We discuss these results, their implications, and future research.</details>

***

**_Positive bias in the ‘Trust in Automated Systems Survey’? An examination of the Jian et al. (2000) scale_**  
Gutzwiller, Robert S.; Chiou, Erin K.; Craig, Scotty D.; Lewis, Christina M.; Lematta, Glenn J.; Hsiung, Chi-Ping  
https://doi.org/10.1177/1071181319631201  
<details><summary>Abstract</summary>Measuring trust in technology is a mainstay in Human Factors research. While trust may not perfectly predict reliance on technology or compliance with alarm signals, it is routinely used as a design consideration and assessment goalpost. Several methods of measuring trust have been employed in the past decades, but one self-report measure stands out due to its popular use, the Trust in Automated Systems Survey (Jian, Bisantz, & Drury, 2000). We conducted a study to assess whether the survey could create biased responses, and found evidence the original scale is in fact skewed toward positive ratings. Assessing the literature revealed the survey has been used in unaltered form across at least 100 different reports and remains frequently administered – therefore, the potential impact of this bias may be widespread. Future directions, considerations, and caveats for our assessment, and for using this scale, are discussed.</details>

***

**_The Consequences of Purposefulness And Human-Likeness on Trust Repair Attempts Made by Self-Driving Vehicles_**  
Kohn, Spencer C.; Momen, Ali; Wiese, Eva; Lee, Yi-Ching; Shaw, Tyler H.  
https://doi.org/10.1177/1071181319631381  
<details><summary>Abstract</summary>Autonomous systems are rapidly gaining the capacity to recognize their own errors and utilize social strategies to mitigate the trust deficit that accompanies those errors. While previous research has catalogued the effects of trust repair attempts in human-human relationships, much remains unknown about the consequences of similar strategies when administered by autonomous systems such as self-driving vehicles. While we tend to treat computers like social actors, autonomous systems may have a wider spectrum of perceived human-likeness and may be subject to different interpretations of the purposefulness of their errors. This paper seeks to understand the consequences of these factors on the effectiveness of trust repair attempts administered by self-driving cars, and the results highlight the importance of considering human-likeness and purposefulness in the design of autonomous systems.</details>

***

**_Risk Surface: A Visualization of Data Sharing Risk for Enterprise Users_**  
St. John, Mark F.; Gustafson, Woodrow; Martin, April; Moore, Ronald A.; Korkos, Christopher A.  
https://doi.org/10.1177/1071181319631011  
<details><summary>Abstract</summary>Enterprises share a wide variety of data with different partners. Tracking the risks and benefits of this data sharing is important for avoiding unwarranted risks of data exploitation. Data sharing risk can be characterized as a combination of trust in data sharing partners to not exploit shared data and the sensitivity, or potential for harm, of the data. Data sharing benefits can be characterized as the value likely to accrue to the enterprise from sharing the data by making the enterprise’s objectives more likely to succeed. We developed a risk visualization concept called a risk surface to support users monitoring for high risks and poor risk-benefit trade-offs. The risk surface design was evaluated in a series of two focus groups conducted with human factors professionals. Across the two studies, the design was improved and ultimately rated as highly useful. A risk surface needs to 1) convey which data, as joined data sets, are shared with which partners, 2) convey the degree of risk due to sharing that data, 3) convey the benefits of the data sharing and the trade-off between risk and benefits, and 4) be easy to scan at scale, since enterprises are likely to share many different types of data with many different partners.</details>

***

**_Feedback on system or operator performance: Which is more useful for the timely detection of changes in reliability, trust calibration and appropriate automation usage?_**  
Lu, Yidu; Sarter, Nadine  
https://doi.org/10.1177/1071181319631345  
<details><summary>Abstract</summary>Creating safe human-machine systems requires that operators can quickly notice changes in system reliability in the interest of trust calibration and proper automation usage. Operators’ readiness to trust a system is determined not only by the performance of the automation but also by their confidence in their own abilities. This study therefore compared the usefulness of feedback on the performance of either agent. The experiment required two groups of ten participants each to perform an automation-assisted target identification task with “Automation Performance Feedback” (APF) or “Operator Performance Feedback” (OPF). Four different scenarios differed with respect to the degree and duration of changes in system reliability. Findings indicate that APF was more effective for supporting timely adjustments of perceived system reliability, especially with large and long reliability changes. Subjective trust ratings and performance were not affected, however, suggesting that these two factors are closely linked and more relevant for automation reliance.</details>

***

**_Trust Engineering for Human-AI Teams_**  
Ezer, Neta; Bruni, Sylvain; Cai, Yang; Hepenstal, Sam J.; Miller, Christopher A.; Schmorrow, Dylan D.  
https://doi.org/10.1177/1071181319631264  
<details><summary>Abstract</summary>Human-AI teaming refers to systems in which humans and artificial intelligence (AI) agents collaborate to provide significant mission performance improvements over that which humans or AI can achieve alone. The goal is faster and more accurate decision-making by integrating the rapid data ingest, learning, and analyses capabilities of AI with the creative problem solving and abstraction capabilities of humans. The purpose of this panel is to discuss research directions in Trust Engineering for building appropriate bi-directional trust between humans and AI. Discussions focus on the challenges in systems that are increasingly complex and work within imperfect information environments. Panelists provide their perspectives on addressing these challenges through concepts such as dynamic relationship management, adaptive systems, co-discovery learning, and algorithmic transparency. Mission scenarios in command and control (C2), piloting, cybersecurity, and criminal intelligence analysis demonstrate the importance of bi-directional trust in human-AI teams.</details>

***

**_Practical Guidance for Evaluating Calibrated Trust_**  
McDermott, Patricia L.; Brink, Ronna N. ten  
https://doi.org/10.1177/1071181319631379  
<details><summary>Abstract</summary>As automation, autonomy, and AI become more prevalent, human factors engineers are called to evaluate whether users trust that automation. We argue that the true question is whether users trust the automation appropriately. In other words, do they trust it as much as it deserves to be trusted? When automation performance decreases, are users aware so they decrease their momentary trust, and more importantly, their reliance on the automation? There are few metrics that focus specifically on calibrated trust, and the trust literature can be daunting for human factors professionals who are not experts in trust. We offer two aids to human factors practitioners tasked with evaluating trust. The first is an easy-to-use calibrated trust framework that simplifies the aspects of trust into Belief, Understanding, Intent, and Reliance. The second is the introduction of Calibration Points, a way to classify situations in which the automation excels or situations in which the automation is degraded. By identifying these Calibration Points, human factors practitioners can evaluate whether human trust is aligned with automation performance. This approach allows the practitioner to leverage the rich set of evaluation techniques that have been developed to evaluate trust.</details>

***

**_The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming_**  
Warden, Toby; Carayon, Pascale; Roth, Emilie M.; Chen, Jessie; Clancey, William J.; Hoffman, Robert; Steinberg, Marc L.  
https://doi.org/10.1177/1071181319631100  
<details><summary>Abstract</summary>The National Academies Board on Human Systems Integration (BOHSI) has organized this session exploring the state of the art and research and design frontiers for intelligent systems that support effective human machine teaming. An important element in the success of human machine teaming is the ability of the person on the scene to develop appropriate trust in the automated software (including recognizing when it should not be trusted). Research is being conducted in the Human Factors community and the Artificial Intelligence (AI) community on the characteristics that software need to display in order to foster appropriate trust. For example, there is a DARPA program on Explainable AI (XAI). The Panel brings together prominent researchers from both the Human Factors and AI communities to discuss the current state of the art, challenges and short-falls and ways forward in developing systems that engender appropriate trust.</details>

***

**_A System Dynamics Model for Human Trust in Automation under Speed and Accuracy Requirements_**  
Hussein, Aya; Elsawah, Sondoss; Abbass, Hussein  
https://doi.org/10.1177/1071181319631167  
<details><summary>Abstract</summary>Research shows that human trust in automation is a key predictor of human reliance on the automation. Several models have been proposed to capture the interplay between trust and reliance and their combined impacts on task performance. Whereas some models assume that trust is affected by automation reliability, others assume that trust is affected by automation speed. In fact, both speed and reliability can be crucial for mission performance, therefore, these models do not represent the interrelationships among automation speed, automation reliability, human decision making, and subsequent effects on mission performance. To address this gap, we propose a system dynamics model which incorporates both the speed and reliability of automation and their combined effects on trust. Our model explicitly represents the speed-accuracy compromise adopted by the subjects to weigh the perceived relative importance of these aspects while evaluating the reliance decision. The model is calibrated and evaluated using data collected from a human experiment in which 33 subjects interacted with an automated aid for swarm supervision in a foraging mission. The simulation results show that the model can closely replicate and predict the experimental data in terms of the reliance rate and the number of targets collected. Model limitations and further efforts for model extension are discussed.</details>

***

**_A Capacity Coefficient Method for Characterizing the Impacts of Automation Transparency on Workload Efficiency_**  
Fallon, Corey K.; Blaha, Leslie M.; Jefferson, Brett; Franklin, Lyndsey  
https://doi.org/10.1177/1071181319631436  
<details><summary>Abstract</summary>Automation can be unreliable. This makes appropriate trust and reliance difficult to calibrate. One solution to building appropriate trust is to increase automation transparency by displaying information to the operator about the technology’s underlying analytical principles. However, displaying this additional information may increase operator workload. The research and development community must balance the competing demands of providing adequate transparency and keeping operator workload low. To investigate the complex effects of transparency on workload, a modeling approach can be used by computing a measure of processing efficiency called the capacity coefficient. We conducted a study to examine the impact of increasing transparency on operator workload using the capacity coefficient. We present the data from one participant with the goal of demonstrating the utility of the capacity coefficient. We discuss how this participant’s data highlights the inferences possible from capacity analysis for measuring the impact of display design and increased transparency on operator workload.</details>

***

**_Public’s acceptance of automated vehicles: the role of initial trust and subjective norm_**  
Zhang, Tingru; Tan, Haibo; Li, Shuling; Zhu, Haoyu; Tao, Da  
https://doi.org/10.1177/1071181319631183  
<details><summary>Abstract</summary>This study aimed to investigate factors that determine public’s acceptance of automated vehicles (AVs) by proposing and empirically testing an AV acceptance model. The model was developed by incorporating two factors, i.e., trust and subjective norm, into the Technology Acceptance Model. The validity of the model was confirmed with a structure equation modeling analysis based on data collected from 395 survey samples. It was found that trust was the most critical determinant of public’s acceptance of AVs and offered a way for other factors (i.e. perceived usefulness and social norm) to impact AV acceptance. Subjective norm was also identified as an important factor in shaping users’ usage intention, at lease in China, a country dominated by collectivistic culture. Findings from this study provide some guidelines in improving AV products to attract users.</details>

***

**_The psychological factors that influence successful technology adoption in the oil and gas industry_**  
Roberts, Ruby; Flin, Rhona  
https://doi.org/10.1177/1071181319631105  
<details><summary>Abstract</summary>To ensure that the full potential of innovative technology is maximised, it is crucial to understand the psychological factors that influence technology adoption in all industrial consumers. The oil and gas (O&G) industry exemplifies industrial consumers’ reluctance to adopt new technology. Our critical incident interviews identified the key psychological factors that influence technology adoption in the O&G industry. These were personality (innovativeness and risk aversion), attitudes (trust, motivations, “not invented here” syndrome and “engineering mindset”), social (subjective norms and self-image), cognitive (risk perception, uncertainty and familiarity, expertise, and previous experiences) and organizational level factors (leadership, management, organisational culture, adoption culture, and rewards system). In combination with future case studies, these results can be used to develop interventions that support the successful introduction and acceptance of new technology not only in O&G but in other high-risk sectors.</details>

***

**_Human-Robot Interaction with Drones and Drone Swarms in Law Enforcement Clearing Operations_**  
Stone, Richard T.; Schnieders, Thomas M.; Push, Kevin A.; Terry, Stephen; Truong, Mary; Seshie, Inshira; Socha, Kathryn  
https://doi.org/10.1177/1071181319631465  
<details><summary>Abstract</summary>Police officers often must work alone in clearing operations, a procedure that involves surveying a building for threats and appropriately responding. A partnership between drone swarms and officers has potential to increase the safety of officers and civilians during these high-stress operations and reduce the risk of harm from hostile persons. This two part study examines aspects of trust, situational awareness, mental demand, performance, and human-robot interaction during law enforcement building clearing operations using either a single drone or a drone swarm. Results indicate that single drone use can increase time for operation, but accuracy and safety of clearing is enhanced. Single drone use saw increased situational awareness, a decrease in number of targets missed, and a moderate level of trust. For drone swarms, results indicate significant differences in mental workload from swarm data feeds compared to single drone feeds but no substantial difference in accuracy of finding targets.</details>

***

**_Light-Based External Human Machine Interface: Color Evaluation for Self-Driving Vehicle and Pedestrian Interaction_**  
Faas, Stefanie M.; Baumann, Martin  
https://doi.org/10.1177/1071181319631049  
<details><summary>Abstract</summary>In today’s road traffic pedestrians seek eye contact with drivers to move along safely. Such communication is no longer possible with self-driving vehicles. Previous research shows that an external Human-Machine-Interface (eHMI) provides an interface between self-driving vehicles and pedestrians. However, recommendations for standardization are still being developed. The study compares the colors white and turquoise for eHMI lamps indicating that the automated driving system is engaged. The colors are evaluated in a street-crossing scenario and a parking lot scenario with a Wizard-of-Oz vehicle equipped with eHMI lamps mounted on top of the vehicle. We conducted questionnaires and structured interviews with N=59 participants to identify eHMI design guidelines. Our research provides evidence that turquoise facilitates pedestrians’ factors like visibility, discriminability, sense of safety and trust higher than white. The results are consistent among traffic scenarios. This paper contributes in formulating research-based design guidelines to improve pedestrian safety.</details>

***

**_Safety Climate in Military Organizations: A Pilot Study of an Adjusted Multi-Domain Instrument_**  
Schüler, M.; Matuszczyk, J. Vega  
https://doi.org/10.1177/1071181319631253  
<details><summary>Abstract</summary>The aim of this pilot study is to adjust the NOSACQ-50 to the work environment of military organizations. NOSACQ-50 is a validated tool successfully used in several organizational domains to measure occupational safety climate (OSC). In general, few studies have been published investigating OSC in military organizations. NOSACQ-50 consists of 50 items across 7 OSC dimensions, i.e. group members’ shared perceptions of: 1) management safety priority, commitment and competence; 2) management safety empowerment; 3) management safety justice; 4) workers’ safety commitment; 5) workers’ safety priority and risk non-acceptance; 6) safety communication, learning, and trust in co-workers’ safety competence; 7) workers’ trust in the efficacy of safety systems. To assess the relevance of the NOSACQ-50 items, a revised version of the instrument was sent to 11 military safety experts. In addition, 19 items pertaining to areas not covered by NOSACQ-50 were validated by the same experts. After contents validation, data from 517 participants from 4 garrisons were collected. The results showed that NOSACQ-50 had acceptable reliability scores (.70-.89.), and the factor structure was confirmed by confirmatory factor analysis (CFA). Principal component analyses (PCA) of the supplementary 19 items showed that 12 items grouped into three dimensions (alpha .74-.91): Management enabling safety performance, personnel’s knowledge of and competence in national laws regulating safety and Unit ethics. In conclusion, preliminary results showed the adjusted NOSACQ-50 instrument could be used to measure OSC in military organizations. However, additional studies must be performed to improve and develop military specific dimensions not covered by NOSACQ-50.</details>

***

**_The Joint Effect of Scientific Knowledge and Photographic Evidence on Expert Witness Credibility_**  
  
https://doi.org/10.1177/1071181319631418  
<details><summary>Abstract</summary>The present study examines the role of scientific and photographic evidence on mock jurors’ perceptions of witness credibility and whether adding such details to an expert witness’s slideshow increases the credibility of that testimony. To assess credibility, 128 undergraduate students were divided across 4 research groups. The students reviewed narrated slideshows of Human Factors expert witness testimony and used the Witness Credibility Scale (Brodsky et al., 2010) to quantify the credibility of that testimony. We hypothesized that adding scientific data and photographs would lead to an increase in perceived credibility. Final results indicate that scientific data did generate a statistically significant increase in perceived credibility, specifically concerning the knowledge and trustworthiness of the witness. Conversely, the inclusion of images did not produce a statistically significant effect on perceived credibility. The results of this study demonstrate that including specialized scientific information in an expert witness’s testimony affects jurors’ overall perception of credibility of the witness.</details>

***

**_Trust Measurement in Human–Automation Interaction: A Systematic Review_**  
Brzowski, Matthew; Nathan-Roberts, Dan  
https://doi.org/10.1177/1071181319631462  
<details><summary>Abstract</summary>This systematic review summarizes current measurements of trust in human-automation interaction. A total of 217 articles were found, and it was determined that 44 articles contained relevant information and met inclusion criteria. The results of the review showed that 75% (n = 33) of articles used subjective measures of trust only, and 41% (n = 18) used researcher-defined methods of measuring trust instead of peer-reviewed and validated scales. Of 10 defined industries, the highest number of articles (n = 14) were assigned to the automotive industry, followed by aviation, military, and security (n = 6). The automated systems studied in relevant articles were decision aids, automated control and navigation systems, and process control systems. This review showed that research of trust in human-automation interaction (1) has the tendency to use subjective measures of trust as the primary or only measure, (2) has the tendency to individually define trust and how it is measured, and (3) is heavily composed of research on automotive automation. Best practices and future research are discussed.</details>

***

**_Trend Analysis of Cyber Security Research Published in HFES Proceedings from 1980 to 2018_**  
Mouloua, Salim A.; Ferraro, James; Mouloua, Mustapha; Matthews, Gerald; Copeland, Robert R.  
https://doi.org/10.1177/1071181319631467  
<details><summary>Abstract</summary>The prevalence of information technology (IT) and widespread use of electronic data storage systems by individuals, corporations, and government branches has prompted investigation into threats to cyber security and data privacy. Within the human factors community, researchers have sought to identify issues related to cyber-attacks and cyber defense and human-centered design. These issues range from placing too much trust in modern technology, to remaining vigilant in the presence of cyber threats such as phishing emails. This study aimed to evaluate trends in cyber security research published in the Proceedings of the Human Factors and Ergonomics Society Annual Meeting from 1980 to 2018. This paper reviewed 114 unique articles, identified using keyword terms related to cyber security. Results were organized, analyzed, and graphed based on topic areas, author and funding agency affiliations, and applicable domains. Analyses indicated that cyber security research has grown exponentially in recent years, with nearly 73% of relevant proceedings papers produced since 2010. Gaps in the literature, directions for future research, and emerging issues related to training are also discussed.</details>

***

**_Smart Home Devices: Promoting User Trust and Protecting User Data_**  
Goh, Lih Seng; Nathan-Roberts, Dan  
https://doi.org/10.1177/1071181319631525  
<details><summary>Abstract</summary>As smart technology is introduced into our homes, new dangers emerge that threaten our safety. New technology is usually subjected to much scrutiny, but smart home devices face even more because they are brought into the home environment, which is focused on safety and privacy. The potential for smart home technology to improve home life is hindered by the fact that potential users face difficulty in trusting and accepting smart home technology. This paper explores different types of trust that can be used to inform strategies, promote trust, reduce threats towards smart home technologies, and overcome challenges in designing these systems and different methods for designing a trustworthy and secure system. To begin designing a trustworthy product that establishes trust between users and smart home technology, manufacturers should use these findings to understand how human beings form trust with new technology.</details>

***

**_Does Team Interaction Exploration Support Resilience in Human Autonomy Teaming?_**  
Lematta, Glenn J.; Johnson, Craig J.; Chiou, Erin K.; Cooke, Nancy J.  
https://doi.org/10.1177/1071181319631492  
<details><summary>Abstract</summary>Project overviewAs a team explores interactions, they may find opportunities to expand and refine teamwork over time. This can have consequences for team effectiveness in normal and unexpected situations (Woods, 2018). Understanding the role of exploratory team interactions may be relevant for human-autonomy team (HAT) resilience in the face of synthetic agent rigidity and lack of anticipation (Demir et al, 2019). Team interaction exploration was defined as team interactions with qualities (e.g. content, communication medium) unique to a team’s interaction history (Cooke et al., 2013; Hills et al., 2015). This study examines the relationship between team interaction exploration and HAT performance in multiple remotely-piloted aerial system (RPAS) reconnaissance missions with degraded conditions. The goal of the task was to take good photos of target waypoints. In this task, three teammates are assigned to specific roles: the navigator plans the route using a digital map, the pilot (synthetic) controls the RPAS and selects target waypoints, and the photographer calibrates camera settings to take a good photo of a target waypoint. The synthetic agent was capable of routine team coordination without explicit team player qualities. Teams communicated via a text-chat interface. Seven unique degraded conditions were injected throughout ten missions. Three automation failures disrupted RPAS status information on the photographer’s or pilot’s display, and three autonomy failures disrupted the synthetic agent’s comprehension of waypoint information or caused the agent to move on to the next target before a photo was taken. Finally, a malicious cyber-attack caused the synthetic agent to fly the RPAS to an enemy occupied waypoint.MethodForty-four participants were recruited from a large southwestern university in pairs and formed teams (22 teams) to participate in this study. These participants were either undergraduate or graduate students. This experiment consisted of ten 40-minute missions in total that were carried out over two sessions separated by one-to two-week intervals. After a baseline mission, an automation and autonomy failure was injected into each mission while the team processed target waypoints. The malicious cyber-attack occurred during the final 20-minutes of the tenth mission. This study collected a several measures including measures of team process, physiological measures, and surveys of teamwork knowledge, trust, workload, and anthropomorphism which are not considered in this study. Exploratory team interaction was operationalized as any text-message unique in content, sender, or recipient that was unrelated to routine coordination of target waypoints. Teams were grouped using k-means clustering by their target processing efficiency, number of overcome roadblocks, and mission performance. The three clusters (K = 3) were comparatively described as low- (N = 7), middle- (N = 7), and high-performing (N = 5) teams. A mixed-factor ANOVA compared the frequency of each team’s exploratory interactions by mission and cluster.Results and discussionHigh-performing teams were distinguished from middle-and low-performing teams in their ability to maintain high levels of overall performance while efficiently processing targets and overcoming many roadblocks. Middle-performing teams were efficient in overcoming roadblocks but had worse mission performance. The findings indicate that 1) high-performing teams explored team interactions more than middle-performing teams, 2) there was no significant difference in exploration frequency between high-and low-performing teams, and 3) teams explored more in the first session than the second session, with the exception of the final mission. Overall, exploratory team interaction differentiated HAT performance in normal and degraded conditions and should be further examined at other levels of interaction, such as content meaning and interaction patterns.</details>

***

**_Understanding Reliance and Trust in Decision Aids for UAV Target Identification_**  
Rogers, Hunter; Khasawneh, Amro; Bertrand, Jeffrey; Chalil, Kapil  
https://doi.org/10.1177/1071181319631172  
<details><summary>Abstract</summary>The use of automation is prevalent in almost every aspect of modern life, and since its inception researchers have been investigating trust in automation. There are many methods of measuring trust. Given that trust means different things to different people and by nature is subjective, most methods are subjective survey assessments (Freedy, DeVisser, Weltman, & Coeyman, 2007; Jian, Bisantz, & Drury, 2000). Many studies have investigated how the reliability of an automated agent or the level of automation changes subjective trust in the automation (Dixon & Wickens, 2006; Du, Zhang, & Yang, 2018; Khasawneh, Rogers, Bertrand, Madathil, & Gramopadhye, 2019; Rogers, Khasawneh, Bertrand, & Madathil, 2017).</details>

***

**_Mind Perception in a Competitive Human-Robot Interaction Game_**  
Currie, Levern Q.; Wiese, Eva  
https://doi.org/10.1177/1071181319631284  
<details><summary>Abstract</summary>Robotic agents are becoming increasingly pervasive in society, and have already begun advancing fields such as healthcare, education, and industry. However, despite their potential to do good for society, many people still feel unease when imaging a future where robots and humans work and live together in shared environments, partly because robots are not generally trusted or ascribed human-like socio-emotional skills such as mentalizing and empathizing. In addition, performing tasks conjointly with robots can be frustrating and ineffective partially due to the fact that neuronal networks involved in action understanding and execution (i.e., the action-perception network; APN) are underactivated in human-robot interaction (HRI). While a number of studies has linked underactivation in APN to reduced abilities to predict a robot’s actions, little is known about how performing a competitive task together with a robot affects one’s own ability to execute or suppress an action. In the current experiment, we use a Go/No-Go task that requires participants to give a response on Go trials and suppress a response on No-Go trials to examine whether the performance of human players is impacted by whether they play the game against a robot believed to be controlled by a human as opposed to being pre-programmed. Preliminary data shows higher false alarm rates on No-Go trials, higher hit rates on Go trials, longer reaction times on Go trials and higher inverse efficiency scores in the human-controlled versus the pre-programmed condition. The results show that mind perception (here: perceiving actions as human-controlled) significantly impacted action execution of human players in a competitive human-robot interaction game.</details>

***

**_Using a Situational Awareness Display to Improve Rider Trust and Comfort with an AV Taxi_**  
Chang, Chun-Cheng; Grier, Rebecca A.; Maynard, Jason; Shutko, John; Blommer, Mike; Swaminathan, Radhakrishnan; Curry, Reates  
https://doi.org/10.1177/1071181319631428  
<details><summary>Abstract</summary>There is evidence that a significant portion of the population does not trust automated vehicles (AV). Research on trust in automation suggests that providing insight into how automation functions via a situational awareness (SA) display improves trust. However, it is unclear how much information is needed to build trust. A study was conducted to evaluate what amount of information improves passenger trust with an AV. After a ride in a simulated AV with one of four SA displays with varying amounts of information or no SA display, participants completed questions about trust and comfort. In addition, participants completed a card sorting task to determine what content they desired to see on the SA display. Our conclusions suggests that an SA display engenders trust and comfort. However, such a display should present a low amount of information. A possible explanation is that high information density displays might be more difficult for participants to interpret.</details>

***

**_Proceedings of the Human Factors and Ergonomics Society 2019 Annual Meeting_**  
  
https://doi.org/10.1177/1071181319631171  
<details><summary>Abstract</summary>This lecture discusses the validation of a new scale, the Trust of Automated Systems Test (TOAST). As we increasingly rely on automated systems to perform tasks, it becomes ever more important to understand the ways in which real operators will use these systems. Trust is a critical determinant of this use. Previous efforts to translate trust, typically an interpersonal human experience, into the domain of artificial cognition and inanimate objects, has met with mixed success. Existing scales tend to anthropomorphize systems and attribute intent to relatively simplistic decision processes. This anthropomorphization has led to operator dissatisfaction with these scales and, consequently, poor data quality. In two studies, I present the validation effort for a scale that avoids anthropomorphizing while still assessing the theoretically-driven underlying dimensions of trust formation. Confirmatory Factor Analyses provide evidence that the TOAST, a nine-item scale measuring operator Understanding of the system and belief in its Efficacy is a valid metric of operator trust of both military and civilian systems.</details>

***

**_Lessons learned from conducting Alzheimer’s disease caregiver mHealth app training and focus group sessions_**  
Brown, Janetta; Kim, Hyung Nam  
https://doi.org/10.1177/1071181319631056  
<details><summary>Abstract</summary>This paper describes the challenges and lessons learned from conducting mobile health application (mHealth app) training and focus group sessions for Alzheimer’s disease caregivers. We review the process behind planning, recruitment, and conducting the training and focus group sessions. Insights presented were gained from preparation for the study, participant discussion, participant user experience with the mHealth apps, and the authors’ observations during both sessions. We found that proper preparation for conducting training and focus group sessions goes far beyond literature reviews and pilot testing. Establishing relationships and preliminary volunteerism with potential support group programs can enrich the experience of the researchers and participants involved in the investigation. The impartation of knowledge is reciprocal, but in this case, mutual trust and respect for the adult learning process were the deciding factors behind the authors’ success.</details>

***

**_The Impact of Virtual Human Voice on Learner Trust_**  
Craig, Scotty D.; Chiou, Erin K.; Schroeder, Noah L.  
https://doi.org/10.1177/1071181319631517  
<details><summary>Abstract</summary>The current study investigates if a virtual human’s voice can impact the user’s trust in interacting with the virtual human in a learning setting. It was hypothesized that trust is a malleable factor impacted by the quality of the virtual human’s voice. A randomized alternative treatments design with a pretest placed participants in either a low-quality Text-to-Speech (TTS) engine female voice (Microsoft speech engine), a high-quality TTS engine female voice (Neospeech voice engine), or a human voice (native female English speaker) condition. All three treatments were paired with the same female virtual human. Assessments for the study included a self-report pretest on knowledge of meteorology, which occurred before viewing the instructional video, and a measure of system trust. The current study found that voice type impacts a user’s trust ratings, with the human voice resulting in higher ratings compared to the two synthetic voices.</details>

***

**_Cross-Cultural Reactions to Peacekeeping Robots Wielding Non-Lethal Weapons_**  
Bliss, James P.; Long, Shelby K.; Karpinsky-Mosley, Nicole  
https://doi.org/10.1177/1071181319631189  
<details><summary>Abstract</summary>Robots may represent a safer alternative to using only human peacekeepers. However, it is unclear how civilian populations will react to such robots given the cultural diversity of affected civilians and the possibility of non-lethal or lethal weapon use by robot peacekeepers. We investigated compliance rates to simulated armed peacekeeping robots by native and expatriate Americans, Chinese, and Japanese. We predicted that compliance to robot demands would vary as a function of lethal weapon availability, robot patrol orders, and cultural background of the participants. One hundred and forty participants representing seven cultural groups performed a virtual shopping task. They were randomly interrupted six times by an anthropomorphic robotic peacekeeper requesting personal items. Participants decided to “comply” or “not comply” with the robot after each interaction and indicated their trust of the robot. Results showed that participants were more likely to comply with robotic peacekeepers wielding backup lethal weapons than those armed with only a non-lethal weapon. Chinese participants residing in America complied most; Americans living in China complied least. Older participants and those with greater nonlethal weapon familiarity showed more positive attitudes towards weapons. These results suggest that lethality, culture, and familiarity may influence interactions with armed robotic peacekeepers.</details>

***

